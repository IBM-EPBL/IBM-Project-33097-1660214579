{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        "Downlad the dataset here:https://drive.google.com/drive/folders/1TB9CAh8Y-gU13Qzb2r1UBvJspfV92Qx8?usp=share_link"
      ],
      "metadata": {
        "id": "q5raXfAyIakS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "qZILL323IlUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'/home/wsuser/work'"
      ],
      "metadata": {
        "id": "ASpyrKFGInVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.7.0\n",
        "!pip install tensorflow==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYL9QDrjIvmA",
        "outputId": "74d994de-b71d-46da-9c3a-92f6d6ade389"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 25.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.19.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.50.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.9.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.38.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.12)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.27.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 25.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten"
      ],
      "metadata": {
        "id": "Zb4J3O8lI22z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Processing"
      ],
      "metadata": {
        "id": "gPF6f6G4KXit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "nXZ58wg2KbBj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Data Authentication"
      ],
      "metadata": {
        "id": "AJ9HPRo0KhLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "2l_lIycqKgKK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Image DataGenerator Functionality To Trainset And Testset"
      ],
      "metadata": {
        "id": "lXDsh9vpK0Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, types\n",
        "import pandas as pd\n",
        "from botocore.client import Config\n",
        "import ibm_boto3\n",
        "\n",
        "def __iter__(self): return 0\n",
        "\n",
        "# @hidden_cell\n",
        "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
        "# You might want to remove those credentials before you share the notebook.\n",
        "cos_client = ibm_boto3.client(service_name='s3',\n",
        "    ibm_api_key_id='GaUZ7L_S7syPXtiQissJAz5E16m9cNj532640UOYD2Hi',\n",
        "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
        "    config=Config(signature_version='oauth'),\n",
        "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
        "\n",
        "bucket = 'imageclassification-donotdelete-pr-v1604oqevxtyin'\n",
        "object_key = 'Dataset.zip'\n",
        "\n",
        "streaming_body_8 = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n",
        "\n",
        "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
        "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
        "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
        "# pandas documentation: http://pandas.pydata.org/"
      ],
      "metadata": {
        "id": "5acIXf0dK6Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile as zf\n",
        "\n",
        "files = zf.ZipFile(BytesIO(streaming_body_8.read()),'r')\n",
        "files.extractall()\n",
        "files.close()"
      ],
      "metadata": {
        "id": "wgpG0vVxK9Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import zipfile\n",
        "unzip=zipfile.ZipFile(BytesIO(streaming_body_8.read()),'r')\n",
        "file_paths=unzip.namelist()\n",
        "for path in file_paths:\n",
        "    unzip.extract(path)"
      ],
      "metadata": {
        "id": "GgdBE9N2K_oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "PFkuTpgkLEX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'/home/wsuser/work'"
      ],
      "metadata": {
        "id": "IysVsiMQLILC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filenames=os.listdir('/home/wsuser/work/Dataset/TRAIN_SET')"
      ],
      "metadata": {
        "id": "KefYBUorLEeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall keras -y\n",
        "!pip uninstall keras-nightly -y\n",
        "!pip uninstall keras-Preprocessing -y\n",
        "!pip uninstall keras-vis -y\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall h5py -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWjgKjZcLPgv",
        "outputId": "384f8fb3-9fdf-4b3d-e50e-12a4fff12841"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.7.0\n",
            "Uninstalling keras-2.7.0:\n",
            "  Successfully uninstalled keras-2.7.0\n",
            "\u001b[33mWARNING: Skipping keras-nightly as it is not installed.\u001b[0m\n",
            "Found existing installation: Keras-Preprocessing 1.1.2\n",
            "Uninstalling Keras-Preprocessing-1.1.2:\n",
            "  Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Found existing installation: keras-vis 0.4.1\n",
            "Uninstalling keras-vis-0.4.1:\n",
            "  Successfully uninstalled keras-vis-0.4.1\n",
            "Found existing installation: tensorflow 2.7.0+zzzcolab20220506150900\n",
            "Uninstalling tensorflow-2.7.0+zzzcolab20220506150900:\n",
            "  Successfully uninstalled tensorflow-2.7.0+zzzcolab20220506150900\n",
            "Found existing installation: h5py 3.1.0\n",
            "Uninstalling h5py-3.1.0:\n",
            "  Successfully uninstalled h5py-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.0.8\n",
        "!pip install h5py==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "5U8k5cmqLUlA",
        "outputId": "565ec133-bb4b-4d33-efaa-d08dd32613c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.0.8\n",
            "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[K     |████████████████████████████████| 276 kB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.7.3)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.6)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_datagen.flow_from_directory(\n",
        "    '/home/wsuser/work/Dataset/TRAIN_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse')\n",
        "\n",
        "x_test = test_datagen.flow_from_directory(\n",
        "    '/home/wsuser/work/Dataset/TEST_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse') \n"
      ],
      "metadata": {
        "id": "LmwoMs7ILdaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 4118 images belonging to 5 classes.\n",
        "\n",
        "Found 929 images belonging to 5 classes."
      ],
      "metadata": {
        "id": "nQe8F1o7LgJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.class_indices)"
      ],
      "metadata": {
        "id": "Nl9A54D2Ljn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'PINEAPPLE': 3, 'WATERMELON': 4}\n"
      ],
      "metadata": {
        "id": "moyUML6tLs-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.class_indices)"
      ],
      "metadata": {
        "id": "5B1zlvwGLsHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'PINEAPPLE': 3, 'WATERMELON': 4}\n"
      ],
      "metadata": {
        "id": "RRvEqGAoLyb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter as c\n",
        "c(x_train .labels)"
      ],
      "metadata": {
        "id": "F_zv8YArL7gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counter({0: 995, 1: 1354, 2: 1019, 3: 275, 4: 475})"
      ],
      "metadata": {
        "id": "mtzfW3G0MB51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "M9P-I-4kMIjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Importing The Model Building Libraries"
      ],
      "metadata": {
        "id": "oAaa7ZsZMLhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense,Flatten\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout "
      ],
      "metadata": {
        "id": "ep-TRakyMOkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Initializing The Model"
      ],
      "metadata": {
        "id": "mtqrdQHjMRgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "03_QkTx5MSiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Adding CNN Layers"
      ],
      "metadata": {
        "id": "mJuFPi6HMWlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Sequential()\n",
        "\n",
        "\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "classifier.add(Flatten())"
      ],
      "metadata": {
        "id": "KS8pJ8YbMXiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Adding Dense Layers"
      ],
      "metadata": {
        "id": "XQzl-bQzMdOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dense(units=5, activation='softmax'))"
      ],
      "metadata": {
        "id": "GzYJUWqHMnc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.summary()"
      ],
      "metadata": {
        "id": "_XgAV3yrMpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Configure The Learning Process"
      ],
      "metadata": {
        "id": "gXtdotwuMvK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) "
      ],
      "metadata": {
        "id": "NSOKdQvLM27p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Train The Model"
      ],
      "metadata": {
        "id": "1nzTyhE8M62X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit_generator(generator=x_train,steps_per_epoch = len(x_train),epochs=20, validation_data=x_test,validation_steps = len(x_test))"
      ],
      "metadata": {
        "id": "3XWbsloXNN7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Saving The Model"
      ],
      "metadata": {
        "id": "SzokjIduNGCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save('nutrition.h5')"
      ],
      "metadata": {
        "id": "OfcgmN_INTnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Testing The Model"
      ],
      "metadata": {
        "id": "mGS8JXUbNVmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf image-classification-model_new.tgz nutrition.h5"
      ],
      "metadata": {
        "id": "Ye8gV1K5NYLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nutrition.h5"
      ],
      "metadata": {
        "id": "QXop2J4CNaMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "hkIIU7zcNcXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset/  image-classification-model_new.tgz  nutrition.h5"
      ],
      "metadata": {
        "id": "30opE0DNNf2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watson-machine-learning-client --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_gQecoX4Nj0_",
        "outputId": "1b954a13-d3d1-4083-9ddf-4d8418d174fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting watson-machine-learning-client\n",
            "  Downloading watson_machine_learning_client-1.0.391-py3-none-any.whl (538 kB)\n",
            "\u001b[K     |████████████████████████████████| 538 kB 21.9 MB/s \n",
            "\u001b[?25hCollecting lomond\n",
            "  Downloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.13-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk\n",
            "  Downloading ibm-cos-sdk-2.12.0.tar.gz (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (2022.9.24)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (1.24.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (0.8.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from watson-machine-learning-client) (1.3.5)\n",
            "Collecting botocore<1.30.0,>=1.29.13\n",
            "  Downloading botocore-1.29.13-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.13->boto3->watson-machine-learning-client) (2.8.2)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.13->boto3->watson-machine-learning-client) (1.15.0)\n",
            "Collecting ibm-cos-sdk-core==2.12.0\n",
            "  Downloading ibm-cos-sdk-core-2.12.0.tar.gz (956 kB)\n",
            "\u001b[K     |████████████████████████████████| 956 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.12.0\n",
            "  Downloading ibm-cos-sdk-s3transfer-2.12.0.tar.gz (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->watson-machine-learning-client) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->watson-machine-learning-client) (2.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->watson-machine-learning-client) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->watson-machine-learning-client) (1.21.6)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.12.0-py3-none-any.whl size=73931 sha256=e53c64a6f76ba273668085d799bead736e1d922b5d80564beb897bde9a71ea2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/94/29/2b57327cf00664b6614304f7958abd29d77ea0e5bbece2ea57\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.12.0-py3-none-any.whl size=562962 sha256=8bcae5dff83c738c355356b7fd223e5e34129c223de5f81da400d0247476c155\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/56/fb/5cd6f4f40406c828a5289b95b2752a4d142a9afb359244ed8d\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.12.0-py3-none-any.whl size=89778 sha256=2b35aea31d39cbef0bf2961a7bd26e58cde799521d6bed16215a47eeed493dfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/79/6a/ffe3370ed7ebc00604f9f76766e1e0348dcdcad2b2e32df9e1\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: urllib3, requests, jmespath, ibm-cos-sdk-core, botocore, s3transfer, ibm-cos-sdk-s3transfer, lomond, ibm-cos-sdk, boto3, watson-machine-learning-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, which is not installed.\u001b[0m\n",
            "Successfully installed boto3-1.26.13 botocore-1.29.13 ibm-cos-sdk-2.12.0 ibm-cos-sdk-core-2.12.0 ibm-cos-sdk-s3transfer-2.12.0 jmespath-0.10.0 lomond-0.3.3 requests-2.28.1 s3transfer-0.6.0 urllib3-1.26.12 watson-machine-learning-client-1.0.391\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning import APIClient\n",
        "\n",
        "wml_credentials={\n",
        "    \"url\" : \"https://us-south.ml.cloud.ibm.com\",\n",
        "    \"apikey\" : \"Y8ioAQxujPdmjRCFur8Q6M4VnKtsAhGQHRQZbwIMyoxj\"\n",
        "}\n",
        "client=APIClient(wml_credentials)"
      ],
      "metadata": {
        "id": "td0QMZH9NuMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client=APIClient(wml_credentials)"
      ],
      "metadata": {
        "id": "KfBs_-I-Nyld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guid_from_space_name(client,space_name):\n",
        "    space=client.spaces.get_details()\n",
        "    return (next(item for item in space['resources'] if item['entity']['name']==space_name)['metadata']['id'])"
      ],
      "metadata": {
        "id": "0BO6vSs9N1sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space_uid=guid_from_space_name(client,'model')\n",
        "print(\"Space UID = \" + space_uid)"
      ],
      "metadata": {
        "id": "AzkETgaTN4G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Space UID = f0e78f3c-2a8d-464f-a1bd-bb372d0f1bb9"
      ],
      "metadata": {
        "id": "3DL4o6qWN5zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.set.default_space(space_uid)"
      ],
      "metadata": {
        "id": "1MzBGChsN8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'SUCCESS'"
      ],
      "metadata": {
        "id": "uJtMI8W6N_mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.software_specifications.list()"
      ],
      "metadata": {
        "id": "vqv4UtAXOEql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "software_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.6\")\n",
        "software_spec_uid"
      ],
      "metadata": {
        "id": "3PwqNX6QOI6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'0062b8c9-8b7d-44a0-a9b9-46c416adcbd9'"
      ],
      "metadata": {
        "id": "wgn0aYDPOLZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ibm_watson_machine_learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJYobq9DONg0",
        "outputId": "222eccaf-a509-4312-f129-c35d4c8a4727"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ibm_watson_machine_learning\n",
            "  Downloading ibm_watson_machine_learning-1.0.257-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (2022.9.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (2.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (1.26.12)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (4.13.0)\n",
            "Collecting ibm-cos-sdk==2.7.*\n",
            "  Downloading ibm-cos-sdk-2.7.0.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 687 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (0.8.10)\n",
            "Requirement already satisfied: pandas<1.5.0,>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from ibm_watson_machine_learning) (1.3.5)\n",
            "Collecting ibm-cos-sdk-core==2.7.0\n",
            "  Downloading ibm-cos-sdk-core-2.7.0.tar.gz (824 kB)\n",
            "\u001b[K     |████████████████████████████████| 824 kB 44.5 MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.7.0\n",
            "  Downloading ibm-cos-sdk-s3transfer-2.7.0.tar.gz (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (0.10.0)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=0.24.2->ibm_watson_machine_learning) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.5.0,>=0.24.2->ibm_watson_machine_learning) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->ibm_watson_machine_learning) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->ibm_watson_machine_learning) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ibm_watson_machine_learning) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ibm_watson_machine_learning) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ibm_watson_machine_learning) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ibm_watson_machine_learning) (3.0.9)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.7.0-py2.py3-none-any.whl size=72563 sha256=fa29bf4c03e80521087397f86a110d6376104966081619158e7154cc163bdbb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/22/bf/e1154ff0f5de93cc477acd0ca69abfbb8b799c5b28a66b44c2\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.7.0-py2.py3-none-any.whl size=501013 sha256=1f81eb052fec884c102170fbcce497e3384ebf07da9330f730880cb46936bec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/a2/e4/c16d02f809a3ea998e17cfd02c13369281f3d232aaf5902c19\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.7.0-py2.py3-none-any.whl size=88622 sha256=05ae85638e9b3d223a91ac7e988ff5259d03a7596aca774e000403eca06706f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b7/14/fbe02bc1ef1af890650c7e51743d1c83890852e598d164b9da\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: docutils, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watson-machine-learning\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: ibm-cos-sdk-core\n",
            "    Found existing installation: ibm-cos-sdk-core 2.12.0\n",
            "    Uninstalling ibm-cos-sdk-core-2.12.0:\n",
            "      Successfully uninstalled ibm-cos-sdk-core-2.12.0\n",
            "  Attempting uninstall: ibm-cos-sdk-s3transfer\n",
            "    Found existing installation: ibm-cos-sdk-s3transfer 2.12.0\n",
            "    Uninstalling ibm-cos-sdk-s3transfer-2.12.0:\n",
            "      Successfully uninstalled ibm-cos-sdk-s3transfer-2.12.0\n",
            "  Attempting uninstall: ibm-cos-sdk\n",
            "    Found existing installation: ibm-cos-sdk 2.12.0\n",
            "    Uninstalling ibm-cos-sdk-2.12.0:\n",
            "      Successfully uninstalled ibm-cos-sdk-2.12.0\n",
            "Successfully installed docutils-0.15.2 ibm-cos-sdk-2.7.0 ibm-cos-sdk-core-2.7.0 ibm-cos-sdk-s3transfer-2.7.0 ibm-watson-machine-learning-1.0.257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_details=client.repository.store_model(model='image-classification-model_new.tgz',meta_props={ \n",
        "    client.repository.ModelMetaNames.NAME:\"CNN\", \n",
        "    client.repository.ModelMetaNames.TYPE:\"keras_2.2.5\", \n",
        "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_spec_uid }) \n",
        "model_id=client.repository.get_model_uid(model_details)"
      ],
      "metadata": {
        "id": "exdKHLHlORkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.repository.download(model_id, 'my_model.tar.gz')"
      ],
      "metadata": {
        "id": "lLfaFUdXOUyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "AFLJQunxOWow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=load_model(\"nutrition.h5\")"
      ],
      "metadata": {
        "id": "tUz4hFY5OZSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "model = load_model(\"nutrition.h5\")"
      ],
      "metadata": {
        "id": "B49QuOtkObX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = np.expand_dims(x,axis = 0)\n",
        "predict_x=model.predict(x) \n",
        "classes_x=np.argmax(predict_x,axis=-1)\n",
        "classes_x"
      ],
      "metadata": {
        "id": "5WAu-QgzOecJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/1 [==============================] - 0s 290ms/step\n",
        "\n",
        "array([0])"
      ],
      "metadata": {
        "id": "26xqPwBbOhTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=['APPLES', 'BANANA', 'ORANGE','PINEAPPLE','WATERMELON']\n",
        "result=str(index[classes_x[0]])\n",
        "result"
      ],
      "metadata": {
        "id": "kSl1cniXOmsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}